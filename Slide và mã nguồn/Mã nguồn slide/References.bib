@INPROCEEDINGS{8614252,
  author={Siami-Namini, Sima and Tavakoli, Neda and Siami Namin, Akbar},
  booktitle={2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)}, 
  title={A Comparison of ARIMA and LSTM in Forecasting Time Series}, 
  year={2018},
  volume={},
  number={},
  pages={1394-1401},
  doi={10.1109/ICMLA.2018.00227}}
@misc{Hochreiter:91,
  added-at = {2008-03-11T14:52:34.000+0100},
  author = {Hochreiter, S.},
  biburl = {https://www.bibsonomy.org/bibtex/21e476a44125e2b2b588d2fafbb5f69b0/idsia},
  citeulike-article-id = {2381309},
  comment = {See www7.informatik.tu-muenchen.de/\~{}hochreit; advisor: J. Schmidhuber},
  interhash = {c89e6c3623f880f8f4fbe62fd0d320ac},
  intrahash = {1e476a44125e2b2b588d2fafbb5f69b0},
  keywords = {juergen},
  priority = {2},
  timestamp = {2008-03-11T14:54:33.000+0100},
  title = {{Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut f\"{u}r Informatik, Lehrstuhl Prof. Brauer, Technische Universit\"{a}t M\"{u}nchen}},
  year = 1991
}
@article{article,
author = {Bengio, Y. and Simard, Patrice and Frasconi, Paolo},
year = {1994},
month = {02},
pages = {157-66},
title = {Learning long-term dependencies with gradient descent is difficult},
volume = {5},
journal = {IEEE transactions on neural networks / a publication of the IEEE Neural Networks Council},
doi = {10.1109/72.279181}
}
@article{LSTM,
author = {Hochreiter, Sepp and Schmidhuber, JÃ¼rgen},
year = {1997},
month = {12},
pages = {1735-80},
title = {Long Short-term Memory},
volume = {9},
journal = {Neural computation},
doi = {10.1162/neco.1997.9.8.1735}
}
@article{Inovation,author={M. SREENIVASAN AND K. SUMATHI},
title={INNOVATION ALGORITHM IN ARMA PROCESS},
journal={Korean J. Comput. & Appl. Math. Vol. 5 (1998), No. 2, pp. 331 - 340}
}
@article{DBLP:journals/corr/ChoMGBSB14,
  author    = {Kyunghyun Cho and
               Bart van Merrienboer and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               Fethi Bougares and
               Holger Schwenk and
               Yoshua Bengio},
  title     = {Learning Phrase Representations using {RNN} Encoder-Decoder for Statistical
               Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1406.1078},
  year      = {2014},
  url       = {http://arxiv.org/abs/1406.1078},
  eprinttype = {arXiv},
  eprint    = {1406.1078},
  timestamp = {Mon, 13 Aug 2018 16:46:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ChoMGBSB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DGRNN,
author = {Yao, Kaisheng and Cohn, Trevor and Vylomova, Katerina and Duh, Kevin and Dyer, Chris},
year = {2015},
month = {08},
pages = {},
title = {Depth-Gated Recurrent Neural Networks}
}
@article{DBLP:journals/corr/KoutnikGGS14,
  author    = {Jan Koutn{\'{\i}}k and
               Klaus Greff and
               Faustino J. Gomez and
               J{\"{u}}rgen Schmidhuber},
  title     = {A Clockwork {RNN}},
  journal   = {CoRR},
  volume    = {abs/1402.3511},
  year      = {2014},
  url       = {http://arxiv.org/abs/1402.3511},
  eprinttype = {arXiv},
  eprint    = {1402.3511},
  timestamp = {Mon, 13 Aug 2018 16:46:50 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KoutnikGGS14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Liu_Hoi_Zhao_Sun_2016, title={Online ARIMA Algorithms for Time Series Prediction}, volume={30}, url={https://ojs.aaai.org/index.php/AAAI/article/view/10257}, abstractNote={ &lt;p&gt; Autoregressive integrated moving average (ARIMA) is one of the most popular linear models for time series forecasting due to its nice statistical properties and great flexibility. However, its parameters are estimated in a batch manner and its noise terms are often assumed to be strictly bounded, which restricts its applications and makes it inefficient for handling large-scale real data. In this paper, we propose online learning algorithms for estimating ARIMA models under relaxed assumptions on the noise terms, which is suitable to a wider range of applications and enjoys high computational efficiency. The idea of our ARIMA method is to reformulate the ARIMA model into a task of full information online optimization (without random noise terms). As a consequence, we can online estimation of the parameters in an efficient and scalable way. Furthermore, we analyze regret bounds of the proposed algorithms, which guarantee that our online ARIMA model is provably as good as the best ARIMA model in hindsight. Finally, our encouraging experimental results further validate the effectiveness and robustness of our method. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Liu, Chenghao and Hoi, Steven C.H. and Zhao, Peilin and Sun, Jianling}, year={2016}, month={Feb.} }
@article{survey1,
author = {Ma, Qihang},
year = {2020},
month = {01},
pages = {01026},
title = {Comparison of ARIMA, ANN and LSTM for Stock Price Prediction},
volume = {218},
journal = {E3S Web of Conferences},
doi = {10.1051/e3sconf/202021801026}
}
@article{survey2,
author = {Adebiyi, Ayodele and Adewumi, Aderemi and Ayo, Charles},
year = {2014},
month = {03},
pages = {1-7},
title = {Comparison of ARIMA and Artificial Neural Networks Models for Stock Price Prediction},
volume = {2014},
journal = {Journal of Applied Mathematics},
doi = {10.1155/2014/614342}
}
@inproceedings{survey3,
author = {Siami Namini, Sima and Tavakoli, Neda and Siami Namin, Akbar},
year = {2018},
month = {12},
pages = {1394-1401},
title = {A Comparison of ARIMA and LSTM in Forecasting Time Series},
doi = {10.1109/ICMLA.2018.00227}
}